## Network Dissection: Quantifying Interpretability of Deep Visual Representations
**Bau et al, 2017**

This paper attempts to quantify and validate the interpretive power of the latent representations of convolutional neural networks.

1. Paper: https://arxiv.org/pdf/1704.05796
2. Code and data: https://netdissect.csail.mit.edu/

### 1. Introduction

#### 1.1. Related Work

Understanding CNN behavior can be achieved by:
1. Visualizing image portions that maximize activations
2. Using backpropagation variants to identify salient image features
3. Isolating network pieces and assessing them on other tasks
4. Etc.

### 2. Network Dissection

#### 2.1. Broden: Broadly and Densely Labeled Dataset

#### 2.2. Scoring Unit Interpretability

### 3. Experiments

#### 3.1. Human Evaluation of Interpretations

#### 3.2. Measurement of Axis-Aligned Interpretability

#### 3.3. Disentangled Concepts by Layer

#### 3.4. Network Architectures and Supervisions

#### 3.5. Training Conditions vs. Interpretability

#### 3.6  Discrimination vs. Interpretability

#### 3.7 Layer Width vs. Interpretability

### 4. Conclusion
