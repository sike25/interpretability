{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "adversaries_solution.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im1DpaOk8cEI",
        "colab_type": "text"
      },
      "source": [
        "## Setup helper code, data, and models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6r8Dm6zwlQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture \n",
        "!pip install robustness\n",
        "!git clone https://github.com/SIDN-IAP/adversaries.git code\n",
        "!mv code/*.py .\n",
        "!wget http://people.csail.mit.edu/tsipras/stuff.zip\n",
        "!unzip stuff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rCjTe77wxTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")  \n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpNDIin3wTm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import basic libraries needed for the exercise (numpy, matplotlib, and torch)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.autonotebook import tqdm\n",
        "import torch as ch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# We also use the robustness library (https://robustness.readthedocs.io/en/latest/) for some \n",
        "# convenient functionality.\n",
        "from robustness.tools.vis_tools import show_image_row\n",
        "\n",
        "import helpers \n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz_7APq-wTnJ",
        "colab_type": "text"
      },
      "source": [
        "# Excercise I: Gradients as model interpretations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehiKhYfgwTnT",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hru871lywTnV",
        "colab_type": "text"
      },
      "source": [
        "For our interpretability experiments, we will use the ImageNet dataset from the ILSVRC challenge. This is a 1000 class dataset, that has played an important role in developing and evaluating deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHkiZebSwTnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creater a dataset, and a loader to access it. We also need to obtain the \n",
        "# input normalization function used during training to use for testing. Finally,\n",
        "# we also get a label map, that tells us what class a corresponding numeric\n",
        "# value corresponds to.\n",
        "in_dataset, in_loader, normalization_function, label_map_IN = helpers.load_dataset(\n",
        "                                                                'imagenet',\n",
        "                                                                 batch_size=5,\n",
        "                                                                 num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFvcfa5MwTnk",
        "colab_type": "text"
      },
      "source": [
        "We can visualize some samples, along with their labels, from the dataset as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxqzdc_QwTnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, (img, targ) = next(enumerate(in_loader))\n",
        "\n",
        "show_image_row([img],\n",
        "               [\"ImageNet Images\"],\n",
        "               tlist=[[label_map_IN[int(t)] for t in targ]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxf5Dp2gwTnv",
        "colab_type": "text"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAdbw4INwTnz",
        "colab_type": "text"
      },
      "source": [
        "Next, we need a model to interpret! PyTorch provides access to a large range of pre-trained deep networks (for a full list, see <https://pytorch.org/docs/stable/torchvision/models.html>). For example, we can load a ResNet18 using the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jbf4IokwTn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "std_model = helpers.load_model('resnet18')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APNP9eKKwTn8",
        "colab_type": "text"
      },
      "source": [
        "### Compute and visualize gradient\n",
        "We will now compute and visualize the gradient of the loss with respect to the input. For every image pixel, the gradient tells us how the loss changes if we vary that pixel slightly. The gradient can be easily computed using a modern auto-diff framework (in our case PyTorch).\n",
        "\n",
        "We have already implemented the gradient computation for you since there can be subtle bugs, but **make sure to check and understand the `get_gradient` method in `helpers.py`**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-sC8A8zwTn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the gradient of the loss, with respect to the input.\n",
        "\n",
        "grad, _ = helpers.get_gradient(std_model, img, targ, normalization_function)\n",
        "\n",
        "# We can then visualize the original image, along with the gradient. Note that the gradient may\n",
        "# not lie within the valid pixel range ([0, 1]), so we need to rescale it using the \n",
        "# `visualize_gradient` function.\n",
        "\n",
        "show_image_row([img, helpers.visualize_gradient(grad)],\n",
        "               [\"Original Image\", \"Gradient\"],\n",
        "               tlist=[[label_map_IN[int(t)] for t in targ],\n",
        "                      [\"\" for _ in targ]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ToBm67BwToG",
        "colab_type": "text"
      },
      "source": [
        "# Exercise II:  Try SmoothGrad and visualize the interpretations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFMP_ILpwToJ",
        "colab_type": "text"
      },
      "source": [
        "Fill in the following skeleton to implement SmoothGrad. Recall that SmoothGrad computes the gradient at multiple nearby points (obtained by adding noise) and averages them.\n",
        "\n",
        "```\n",
        "def smooth_grad(mod, im, targ, normalization, Nsamples, stdev):\n",
        "    it = tqdm(range(Nsamples), total=Nsamples)\n",
        "    total_grad = 0\n",
        "    for _, n in it:\n",
        "        ...\n",
        "    return total_grad / Nsamples\n",
        "```\n",
        "    \n",
        "Then, try using SmoothGrad to interpret a standard model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcOr2Qd0wToM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_grad(mod, im, targ, normalization,\n",
        "                Nsamples, stdev):\n",
        "    # Instead of taking the gradient of a single image, we will take gradients\n",
        "    # at a bunch of neighborhood points and average their gradients.\n",
        "    \n",
        "    it = tqdm(range(Nsamples), total=Nsamples)\n",
        "\n",
        "    total_grad = 0\n",
        "    for _ in it:\n",
        "        pass # FILL THIS IN\n",
        "    \n",
        "    # Return average gradient\n",
        "    return total_grad / Nsamples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfbaNXAywToa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute and visualize smoothed gradient\n",
        "sgrad = smooth_grad(std_model, img, targ, normalization_function,\n",
        "                    Nsamples=100, stdev=0.3)\n",
        "\n",
        "show_image_row([img, helpers.visualize_gradient(sgrad)],\n",
        "              [\"Original Image\", \"SmoothGrad\"],\n",
        "               tlist=[[label_map_IN[int(t)] for t in targ],\n",
        "                     [\"\" for _ in targ]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZPPaZnJwToi",
        "colab_type": "text"
      },
      "source": [
        "# Excercise III: Adversarial examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOIpO-kDwTom",
        "colab_type": "text"
      },
      "source": [
        "It turns out that one can add an *imperceptible* perturbation to any input image and a standard (highly accurate) classifier will misclassify it (or classify it as an adversarially chosen class). These are known as \"adversarial examples\".\n",
        "\n",
        "**Finding adversarial examples:** The idea is pretty simple: given a target class (t), we want to find a perturbation ($\\delta$) that when added to the input (x) maximizes the likelihood of the target class. At the same time, we want the perturbation to be small: for example lie in a tiny L2 ball around the image (norm computed over pixels). Basically, we want to find a $\\delta$ such that\n",
        "\n",
        "$\\delta = argmax_{||\\delta||_2 \\leq \\epsilon} L(x + \\delta, t; \\theta)$\n",
        "\n",
        "\n",
        "To maximize this objective without violating the constraint, we use projected gradient descent (PGD, see <https://arxiv.org/abs/1706.06083>). \n",
        "\n",
        "We already implemented PGD since there are a few subtle details but **read and understand the `L2PGD` method from `helpers.py`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGQnKQtwTop",
        "colab_type": "text"
      },
      "source": [
        "### Try it yourself! \n",
        "\n",
        "\n",
        "First choose a target class for every input. (Note that you have a batch of inputs, so you could try different targets for different inputs.)\n",
        "\n",
        "Next, there are a couple of parameters that you need to choose: \n",
        "1. `eps`: maximum size of the perturbation in terms of L2 norm. For e.g., eps=2 implies that $||\\delta||_2 \\leq 2$\n",
        "2. `Nsteps`: number of (projected) gradient descent to perform\n",
        "3. `step_size`: size of each step of (projected) gradient descent\n",
        "\n",
        "Try varying these parameters and see what happens.\n",
        "\n",
        "Finally, we will evaluate the model on these adversarial examples and visualize images/predictions on the original (unperturbed) inputs, along with the corresponding adversarial examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flfH5FGiwTo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select target class\n",
        "TARGET = 324 # cabbage butterfly but feel free to change\n",
        "\n",
        "print(f\"Target class: {label_map_IN[TARGET]}\")\n",
        "\n",
        "target_class = TARGET * ch.ones_like(targ)\n",
        "\n",
        "# Create adversarial examples\n",
        "adv_ex = helpers.L2PGD(std_model, img, target_class, normalization_function,\n",
        "                       step_size=0.5, Nsteps=20, \n",
        "                       eps=1.25, targeted=True)\n",
        "# Evaluate model\n",
        "with ch.no_grad():\n",
        "    logits = helpers.forward_pass(std_model, \n",
        "                                      adv_ex, \n",
        "                                      normalization_function)\n",
        "    pred_label = logits.argmax(dim=1)\n",
        "\n",
        "show_image_row([img, adv_ex], \n",
        "               ['Original image', 'Adv. Example'],\n",
        "               tlist=[[label_map_IN[int(t)] for t in label] \\\n",
        "                      for label in [targ, pred_label]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKxMtfLFwTpM",
        "colab_type": "text"
      },
      "source": [
        "# Excercise IV: Playing with robust models\n",
        "\n",
        "There has been a lot of research towards models that are robust to these perturbations, i.e., so-called *robust models*. Here, we will explore the behavior of a robust model. See <http://gradientscience.org/robust_opt_pt1/> for details on training these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pzzr3RcwTpN",
        "colab_type": "text"
      },
      "source": [
        "### Loading a robust model\n",
        "\n",
        "For our study today, we will use a pre-trained robust model. We trained this model (ResNet50) on a 9-class subset of the ImageNet dataset. (Developing good robust models for the more complex 1000 class version is still an active area of research.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGepKLXowTpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the \"Restricted\" ImageNet dataset\n",
        "restricted_imagenet_ds, rin_loader, normalization_function, label_map_RIN = \\\n",
        "            helpers.load_dataset('restricted_imagenet', batch_size=5, num_workers=1)\n",
        "\n",
        "# Load a pre-trained robust model\n",
        "robust_model = helpers.load_model('robust', restricted_imagenet_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o80KaBiwTpU",
        "colab_type": "text"
      },
      "source": [
        "### Can adversarial examples fool a robust model?\n",
        "\n",
        "We can now try to fool the robust model using the same procedure as before. Does it succeed? Try varying the attack parameters and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLhnq4bIwTpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load images from the Restricted ImageNet dataset\n",
        "_, (img, targ) = next(enumerate(rin_loader))\n",
        "\n",
        "# Then we choose a target label for the attack.\n",
        "TARGET = 3\n",
        "\n",
        "print(f\"Target class: {label_map_RIN[TARGET]}\")\n",
        "\n",
        "target_class = TARGET * ch.ones_like(targ)\n",
        "\n",
        "# Create adversarial examples\n",
        "adv_ex = helpers.L2PGD(robust_model, img, target_class, normalization_function,\n",
        "                       step_size=0.5, Nsteps=20, eps=1.25, targeted=True)\n",
        "\n",
        "# Evaluate model predictions\n",
        "with ch.no_grad():\n",
        "    logit = helpers.forward_pass(robust_model, adv_ex, normalization_function)\n",
        "    pred_label = logit.argmax(dim=1)\n",
        "\n",
        "# Visualize adversarial examples\n",
        "\n",
        "show_image_row([img, adv_ex], \n",
        "               ['Original image', 'Adv. Example'],\n",
        "               tlist=[[label_map_RIN[int(t)] for t in label] \\\n",
        "                      for label in [targ, pred_label]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6MQ2LhNwTpa",
        "colab_type": "text"
      },
      "source": [
        "# Excercise V: Changing the prediction of a robust model\n",
        "\n",
        "We know that robust models are not easily fooled by adversarial examples. This tells us that one cannot change the prediction of a robust model using imperceptible L2 perturbations to the input (in contrast to standard models). How can we then modify the input to make the robust model predict a different class?\n",
        "\n",
        "Try creating adversarial examples as before, but with a larger eps. Our hope is that by increasing the size of the perturbation set, we can find a perturbation that actually causes the model to change its prediction? What do the perturbed inputs, i.e., \"*large epsilon adversarial examples*\" look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVHQ_K1cwTpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET = 5\n",
        "\n",
        "print(f\"Target class: {label_map_RIN[TARGET]}\")\n",
        "\n",
        "target_class = TARGET * ch.ones_like(targ)\n",
        "\n",
        "im_targ = helpers.L2PGD(robust_model, img, target_class, normalization_function,\n",
        "                        step_size=5, Nsteps=20, eps=100, targeted=True)\n",
        "\n",
        "# Evaluate model predictions\n",
        "with ch.no_grad():\n",
        "    logit = helpers.forward_pass(robust_model, im_targ, normalization_function)\n",
        "    pred_label = logit.argmax(dim=1)\n",
        "\n",
        "# Visualize\n",
        "show_image_row([img, im_targ],\n",
        "              ['Original image', 'Large eps \\n adv. example'],\n",
        "               tlist=[[label_map_RIN[int(t)] for t in label] \\\n",
        "                      for label in [targ, pred_label]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muw1LsnFwTpm",
        "colab_type": "text"
      },
      "source": [
        "# Excercise VI: Interpretations for robust models\n",
        "\n",
        "Based on the previous experiment, we know that large-eps adv. examples change salient image features.\n",
        "\n",
        "Does this mean that the features that robust models rely on are more human-aligned in a sense?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EGWPx33wTpo",
        "colab_type": "text"
      },
      "source": [
        "### VI.I Let's start by looking at their gradients.\n",
        "\n",
        "What do the gradients of robust models look like? How do they compare to the gradients of a standard model and the output of SmoothGrad?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LwKOP8_wTpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get gradient of the loss with respect to the input\n",
        "grad_rob, _ = helpers.get_gradient(robust_model, img, targ, normalization_function)\n",
        "\n",
        "# Visualize gradient\n",
        "show_image_row([img, helpers.visualize_gradient(grad_rob)],\n",
        "              [\"Original Image\", \"Gradient\"],\n",
        "              tlist=[[label_map_RIN[int(t)] for t in targ],\n",
        "                     [\"\" for _ in targ]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zos8U_PGwTpu",
        "colab_type": "text"
      },
      "source": [
        "### VI.2 Feature Visualization\n",
        "\n",
        "Another popular interpretability technique is known as feature visualization. Here, the goal is to find an input that maximizes a feature (a particular neuron in the deep network), instead of just trying to maximize the loss (as we did before with gradients).\n",
        "\n",
        "You could now try to implement feature visualization yourself. For instance, the following function gives you, for specific inputs, the model's feature vector (the layer before the final linear classifier). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgjX_iu4wTpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the feature representation from the model\n",
        "with ch.no_grad():\n",
        "    feats = helpers.get_features(robust_model, img, normalization_function)\n",
        "    print(f\"Dimensions of the feature vector: {feats.shape[1]}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_stifSoWwTpy",
        "colab_type": "text"
      },
      "source": [
        "#### Implement a loss function to perform feature visualization\n",
        "\n",
        "Fill in the skeleton below to create a feature visualization loss function. Our goal is to maximize the `feature_number` coordinate of the feature vector.\n",
        "\n",
        "```\n",
        "def feature_maximization_loss(mod, im, feature_number, normalization_function):\n",
        "    feature_vector = helpers.get_features(mod, im, normalization) # Get features for input\n",
        "    relevant_coordinate = ch.gather(feature_vector, 1, feature_number[:, None]) \n",
        "    loss = ?\n",
        "    return loss\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GoOX7XdwTp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature visualization loss: Try to find an input that maximizes a specific feature\n",
        "\n",
        "def feature_maximization_loss(mod, im, feature_number, normalization):\n",
        "    # Get feature vector for inputs\n",
        "    fr = helpers.get_features(mod, im, normalization)\n",
        "    # We will maximize the `targ` coordinate of the feature vector for every input\n",
        "    loss = 'FILL THIS IN'\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riNKdTw2wTp3",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize the results of feature visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lISBOCXFwTp5",
        "colab_type": "text"
      },
      "source": [
        "You can then supply the `feature_maximization_loss` to the `custom_loss` argument input in `helpers.L2PGD`, and maximize it using the following snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJoDU94BwTp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chose a feature to visualize\n",
        "FEATURE = 200 # should be less than the dimension from before\n",
        "\n",
        "target_feature = FEATURE * ch.ones_like(targ)\n",
        "\n",
        "# Maximize feature \n",
        "im_f = helpers.L2PGD(robust_model, img, target_feature, normalization_function,\n",
        "                              step_size=5, Nsteps=20, eps=1000, \n",
        "                              custom_loss=feature_maximization_loss, \n",
        "                              targeted=False)\n",
        "\n",
        "# Visualize results\n",
        "show_image_row([img, im_f],\n",
        "               [\"Original Image\", f\"Maximize Feature #{FEATURE}\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgsorIl7wTqP",
        "colab_type": "text"
      },
      "source": [
        "# Bonus Excercise I: Try feature visualization for robust models starting from noise rather than images\n",
        "\n",
        "What if we try feature visualization starting from noise?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep5J7OdBwTqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a \"noise\" image\n",
        "noise_img = ch.clamp(ch.randn_like(img) + 0.5, 0, 1)\n",
        "\n",
        "FEATURE = 205\n",
        "target_feature = FEATURE * ch.ones_like(targ)\n",
        "im_f = helpers.L2PGD(robust_model, noise_img, target_feature, normalization_function,\n",
        "                     step_size=5, Nsteps=200, eps=1000, \n",
        "                     custom_loss=feature_maximization_loss, \n",
        "                     targeted=False)\n",
        "\n",
        "show_image_row([noise_img, im_f],\n",
        "                [\"Original Image\", f\"Maximize Feature #{FEATURE}\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynuNVTUpZsQV",
        "colab_type": "text"
      },
      "source": [
        "# Bonus Excercise II: Try feature visualization for standard models \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWqQx9OCSGyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chose a feature to visualize\n",
        "FEATURE = 200 # should be less than 512 for ResNet18\n",
        "\n",
        "_, (im, targ) = next(enumerate(in_loader))\n",
        "\n",
        "target_feature = FEATURE * ch.ones_like(targ)\n",
        "\n",
        "# Maximize feature \n",
        "im_f = helpers.L2PGD(std_model, img, target_feature, normalization_function,\n",
        "                      step_size=5, Nsteps=20, eps=1000, \n",
        "                      custom_loss=feature_maximization_loss, \n",
        "                      targeted=False)\n",
        "\n",
        "# Visualize results\n",
        "show_image_row([img, im_f],\n",
        "               [\"Original Image\", f\"Maximize Feature #{FEATURE}\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyPEBxgBaEXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "|"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}