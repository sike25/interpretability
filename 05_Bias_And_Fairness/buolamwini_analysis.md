## Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification

**Buolamwini et al, 2018**

This paper characterizes the skin type and gender distributions of two benchmark datasets 
(IJB-A and Adience) and finds lighter skin samples the majority.
It then presents a new dataset with more even distributions. 
It also looks into the performance of three commerical vision classifiers 
and finds that error rates differ significantly by gender and skin type.

Paper: https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf

### 1. Introduction

AI is being used in increasingly more crucial work, 
and there is risk for serious harm from incorrect and confident predictions.

Models trained on biased datasets tend to exhibit these biases. 
For example, Word2Vec embeddings shown to have small distances between words like "man" and "computer programmer"
and words like "woman" and "homemaker".

Work on creating fairer algorithms has not been done much in 
computer vision even though the potential for harm in the field is considerable
given its applications in skin disease recognition and law enforcement.

### 2. Related Work

**Automated Facial Analysis**

### 3. Intersectional Benchmark

**3.1. Rationale for Phenotypic Labeling**

**3.2. Existing Benchmark Selection Rationale**

**3.3. Creation of Pilot Parliaments Benchmark**

**3.4. Intersectional Labeling Methodology**

**3.5. Fitzpatrick Skin Type Comparison**

### 4. Commercial Gender Classification Audit

**4.1. Key Findings on Evaluated Classifiers**

**4.2. Commercial Gender ClassifierSelection: Microsoft, IBM, Face++**

**4.3. Evaluation Methodology**

**4.4. Audit Results**

**4.5. Analysis of Results**

**4.6. Accuracy Metrics**

**4.7. Data Quality and Sensors**



### 5. Conclusion








---

### Reflection      

**What are the strengths?** 


**What are the weaknesses?**      


**What are some significant follow up work from this paper? How do they differ from this paper?**    
